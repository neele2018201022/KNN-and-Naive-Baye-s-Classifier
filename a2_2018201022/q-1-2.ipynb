{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "eps = np.finfo(float).eps\n",
    "from numpy import log2 as log\n",
    "def loadfile():\n",
    "        df =pd.read_csv('/home/neelesh/Downloads/data.csv',header=None, names=(['A','B','C','D','E','F','G','H','I','Res','K','L','M','N']))\n",
    "        X=df[['B','C','D','E','F','G','H','I','K','L','M','N']]\n",
    "        Y=df[['Res']]   \n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state =1)\n",
    "        X_train=X_train.reset_index(drop=True)\n",
    "        Y_train=Y_train.reset_index(drop=True)\n",
    "        X_test=X_test.reset_index(drop=True)\n",
    "        Y_test=Y_test.reset_index(drop=True)\n",
    "        df=pd.concat([X_train,Y_train], axis=1) \n",
    "        testSet=pd.concat([X_test,Y_test], axis=1)\n",
    "        return df,X_train, X_test, Y_train, Y_test,testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmean(arr):\n",
    "        return sum(arr)*1.0/(len(arr))\n",
    "def getstdev(arr):\n",
    "        avg = sum(arr)*1.0/float(len(arr))\n",
    "        variance = sum([pow(x-avg,2) for x in arr])/(float(len(arr)-1)+eps)\n",
    "        return math.sqrt(variance)\n",
    "def helperfunc_mean(df,temp):\n",
    "        return [(getmean(df[attribute]), getstdev(df[attribute])) for attribute in list(temp)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmeandev(df):\n",
    "        att={}\n",
    "        info={}\n",
    "        attValue = np.unique(df['Res'])\n",
    "        for value in attValue:\n",
    "                att[value]=[]\n",
    "                att[value]=df[df['Res'] == value].reset_index(drop=True)\n",
    "        temp=df.drop(['Res'], axis=1)\n",
    "        attValue1 = list(temp)\n",
    "        for value in attValue:\n",
    "                info[value]=[]\n",
    "                info[value] =helperfunc_mean(att[value],temp)\n",
    "        return info    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcGaussianProb(df,meandev,test):\n",
    "        res= {}\n",
    "        attValue = np.unique(df['Res'])\n",
    "        for value in attValue:\n",
    "                res[value]=1\n",
    "        for value in attValue:\n",
    "                for i in range(0,len(meandev[value])):\n",
    "                        mean, stdev = meandev[value][i]\n",
    "                        exponent = math.exp(-(math.pow(test[i]-mean,2)/(2*math.pow(stdev,2)+eps)))\n",
    "                        res[value] *= (1/(eps+(math.sqrt(2*math.pi)*stdev)))*exponent\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df,meandev,test):\n",
    "        res= calcGaussianProb(df,meandev,test)\n",
    "        ans=None\n",
    "        attValue = np.unique(df['Res'])\n",
    "        for value in attValue:\n",
    "            if (ans is None or res[value]>prob):\n",
    "                prob=res[value]\n",
    "                ans=value\n",
    "        return ans\n",
    "    \n",
    "def help_predictor(df,meandev, testSet):\n",
    "        res= []\n",
    "        for i in range(len(testSet)):\n",
    "                    result = predict(df,meandev, testSet.iloc[i])\n",
    "                    res.append(result)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall_precision(original,res):\n",
    "        TP=0\n",
    "        FP=0\n",
    "        TN= 0\n",
    "        FN= 0\n",
    "        f1_score=0\n",
    "        for i in range(0, len(original)):\n",
    "\n",
    "                if res[i] == 1:\n",
    "                    if res[i] == original[i]:\n",
    "                        TP+= 1\n",
    "                    else:\n",
    "                        FP+= 1\n",
    "                else:\n",
    "                    if res[i] == original[i]:\n",
    "                        TN+= 1\n",
    "                    else:\n",
    "                        FN+= 1\n",
    "\n",
    "        precision=0\n",
    "        recall=0\n",
    "        if(TP!=0 or TN!=0):\n",
    "                accuracy = (TP+TN)*1.0/(TP + TN +FP +FN)\n",
    "        if(TP!=0):\n",
    "                precision = TP*1.0/(TP + FP)\n",
    "                recall = TP*1.0/(TP + FN)\n",
    "                f1_score = 2 / ((1 / precision) + (1 / recall))\n",
    "        print \"True +ve=\",TP,\"True -ve=\",TN,\"False +ve=\",FP,\"False -ve=\",FN                    \n",
    "\n",
    "        return accuracy*100, precision*100, recall*100,f1_score*100    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True +ve= 77 True -ve= 708 False +ve= 97 False -ve= 18\n",
      "Accuracy: 87.2222222222%\n",
      "Precison: 44.2528735632%\n",
      "Recall: 81.0526315789%\n",
      "F1 score: 57.249070632%\n"
     ]
    }
   ],
   "source": [
    "def show():\n",
    "        df,X_train, X_test, Y_train, Y_test,testSet=loadfile()\n",
    "        meandev= getmeandev(df)\n",
    "#         if(len(sys.argv)>1):\n",
    "#             Test_Filename=sys.argv[1]\n",
    "#             testdf =pd.read_csv(Test_Filename,header=None, names=(['A','B','C','D','E','F','G','H','I','Res','K','L','M','N']))\n",
    "#             X_test=testdf[['B','C','D','E','F','G','H','I','K','L','M','N']] \n",
    "#             Y_test=testdf[['Res']]\n",
    "#             testSet=pd.concat([X_test,Y_test], axis=1)\n",
    "        \n",
    "        predictions = help_predictor(df,meandev, testSet)\n",
    "        testarr=np.array(testSet['Res'])\n",
    "        a,p,r,f = calculate_recall_precision(testarr, predictions)\n",
    "        print('Accuracy: {0}%'.format(a))\n",
    "        print('Precison: {0}%'.format(p))\n",
    "        print('Recall: {0}%'.format(r))\n",
    "        print('F1 score: {0}%'.format(f))\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
